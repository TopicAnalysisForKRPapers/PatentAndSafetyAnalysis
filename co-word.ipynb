{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T12:22:54.383577Z",
     "start_time": "2023-05-03T12:22:38.264765Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def document_search(document, predicate):\n",
    "    #returns first matched sentence\n",
    "    for sentence in document:\n",
    "        words = sentence.split()\n",
    "        if predicate(words):\n",
    "            return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T12:22:56.770071Z",
     "start_time": "2023-05-03T12:22:54.413068Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from wordcloud import WordCloud\n",
    "import operator\n",
    "from nltk import bigrams\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import chardet\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "csv.field_size_limit(1<<21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T12:22:56.854323Z",
     "start_time": "2023-05-03T12:22:56.771931Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def allowed_characters(string: str) -> str:\n",
    "    # returns only allowed characters and convert else to space\n",
    "    # use regex\n",
    "    return re.sub(r'[^가-힣.]', ' ', string)\n",
    "\n",
    "def remove_single_character(string: str) -> str:\n",
    "    # remove single character\n",
    "    return re.sub(r'\\b\\w\\b', '', string)\n",
    "\n",
    "def prune(txt_file='result.txt', filename='result.csv',splitted_expected_length=4,target_column=3,truncate_threshold=10, max_lines=-1, start_line=0 ):\n",
    "    \"\"\"\n",
    "    prune txt file to csv file\n",
    "    :param txt_file: txt file to prune\n",
    "    :param filename: csv file to save\n",
    "    :param splitted_expected_length: expected length of splitted line, if not equal, skip\n",
    "    :param target_column: target column to prune\n",
    "    :param truncate_threshold: threshold to prune in word counter\n",
    "    :param max_lines: max lines to prune\n",
    "    :param cur_line: current line\n",
    "    \"\"\"\n",
    "    word_counter = Counter()\n",
    "    skipped_line_idxs = set()\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"processing txt file to csv file\")\n",
    "        with open(txt_file, 'r', encoding='utf-8') as txtfile:\n",
    "            total_lines = sum(1 for line in txtfile)\n",
    "        with open(filename, 'w', encoding='utf-8', newline='') as csv_output:\n",
    "            csv_writer = csv.writer(csv_output)\n",
    "            with open(txt_file, 'r', encoding='utf-8') as txtfile:\n",
    "                cur_line = 0\n",
    "                for line in tqdm(txtfile, total=total_lines):\n",
    "                    cur_line += 1\n",
    "                    if cur_line < start_line:\n",
    "                        continue\n",
    "                    splits = line.split('\\t')\n",
    "                    if len(splits) != splitted_expected_length:\n",
    "                        skipped_line_idxs.add(cur_line)\n",
    "                        continue\n",
    "                    # remove \\n\n",
    "                    splits[-1] = splits[-1].replace('\\n', '')\n",
    "                    # remove special characters\n",
    "                    splits = [allowed_characters(s) for s in splits]\n",
    "                    splits = [remove_single_character(s) for s in splits]\n",
    "                    # apply splits to word counter\n",
    "                    word_counter.update(splits[target_column].split())\n",
    "                    # write to csv\n",
    "                    csv_writer.writerow(splits)\n",
    "                    if cur_line == max_lines:\n",
    "                        break\n",
    "        with open('skipped_line_idxs.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join((str(x) for x in skipped_line_idxs)))\n",
    "\n",
    "    # prune and save to csv\n",
    "    file_except_extension = os.path.splitext(filename)[-2]\n",
    "    if not os.path.exists(file_except_extension + '_pruned.csv'):\n",
    "        print(\"pruning csv file\")\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            with open(file_except_extension + '_pruned.csv', 'w', encoding='utf-8', newline='') as f2:\n",
    "                writer = csv.writer(f2)\n",
    "                for row in reader:\n",
    "                    if len(row) < splitted_expected_length:\n",
    "                        continue\n",
    "                    line = row[target_column]\n",
    "                    # apply threshold from word counter\n",
    "                    line = ' '.join([word for word in line.split(' ') if word_counter[word] > truncate_threshold])\n",
    "                    row[target_column] = line\n",
    "                    if len(line) > 0:\n",
    "                        writer.writerow(row)\n",
    "\n",
    "txt_file = 'source.txt'\n",
    "# convert txt file to .csv file, and save it\n",
    "# we expect 20xxxxxx source text\n",
    "# splits with \\t\n",
    "\n",
    "\n",
    "# process kookbang_all.txt\n",
    "prune(txt_file=txt_file, splitted_expected_length=1, target_column=0, filename = 'source.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "processed = 'source.csv'\n",
    "pruned = 'source_pruned.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compress rate\n",
    "rate = os.path.getsize(pruned) / os.path.getsize(processed)\n",
    "print(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T12:22:56.889274Z",
     "start_time": "2023-05-03T12:22:56.854323Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test csv file with index 3\n",
    "with open(pruned, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    lines = 3\n",
    "    for rows in reader:\n",
    "        try:\n",
    "            rows[0]\n",
    "        except IndexError:\n",
    "            print(lines)\n",
    "        lines-=1\n",
    "        if lines == 0:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T12:23:12.204660Z",
     "start_time": "2023-05-03T12:22:56.897977Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import treform as ptm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from ExampleManager import PathManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T12:23:12.204660Z",
     "start_time": "2023-05-03T12:23:12.146176Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T12:23:12.738301Z",
     "start_time": "2023-05-03T12:23:12.183703Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['JAVA_HOME'] = r'C:\\Program Files\\Java\\jdk-16.0.2'\n",
    "#os.environ['GIT_PYTHON_GIT_EXECUTABLE'] = r'C:\\Program Files\\Git\\bin\\git.exe'\n",
    "#import git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T12:23:35.958642Z",
     "start_time": "2023-05-03T12:23:12.713706Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test packages\n",
    "str(PathManager('../stopwords/stopwordsKor.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T12:23:38.745794Z",
     "start_time": "2023-05-03T12:23:35.956057Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class StopwordFilterBeta:\n",
    "    \"\"\"\n",
    "    Stopword filter, removes if length is 1, or if it is in stopwords / with prefix\n",
    "    \"\"\"\n",
    "    IN_TYPE = [list, str]\n",
    "    OUT_TYPE = [list, str]\n",
    "\n",
    "    def __init__(self, stopwords = [], file = None):\n",
    "        if file:\n",
    "            stopwords = stopwords + [line.strip() for line in open(file, encoding='utf-8')]\n",
    "        self.stopwords = set(stopwords)\n",
    "        self.stopwordsPrefix = ('http', 'https', 'ftp', 'git', 'thatt')\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        #any(e for e in test_list if e.startswith('three') or e.endswith('four'))\n",
    "        return [i for i in args[0] if len(i) > 1 and i.lower() not in self.stopwords and (i.lower().startswith(tuple(p for p in self.stopwordsPrefix)) == False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T12:24:24.701894Z",
     "start_time": "2023-05-03T12:24:19.581856Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "if not os.path.exists(\"documents.pkl\"):\n",
    "    print(\"processing corpus to word histogram\")\n",
    "    corpus = ptm.CorpusFromCSVFile(pruned, 0)\n",
    "    pipeline = ptm.Pipeline(ptm.splitter.NLTK(),\n",
    "                            ptm.tokenizer.Komoran(),\n",
    "                            ptm.helper.POSFilter('NN*'),\n",
    "                            ptm.helper.SelectWordOnly(),\n",
    "                            StopwordFilterBeta(file=str(PathManager('../stopwords/stopwordsKor.txt')))\n",
    "                            )\n",
    "    result = pipeline.processCorpus(tqdm(corpus))\n",
    "    documents = []\n",
    "    for doc in result:\n",
    "        for sent in doc:\n",
    "            sentence = ' '.join(sent)\n",
    "            sentence = re.sub('[^가-힣_ ]+', '', sentence)\n",
    "            sentence = sentence.strip()\n",
    "            if len(sentence) > 0:\n",
    "                documents.append(sentence)\n",
    "    print(len(documents))\n",
    "    with open('documents.pkl', 'wb') as f:\n",
    "        pickle.dump(documents, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('documents.pkl', 'rb') as f:\n",
    "    documents = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document_arr = [tqdm((d.split(' ') for d in documents), total = len(documents)),]\n",
    "from tqdm import tqdm\n",
    "co = ptm.CooccurrenceManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "# clear tqdm (jupyter notebook)\n",
    "tqdm._instances.clear()\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.util import bigrams\n",
    "import operator\n",
    "def computeCooccurence(iterable:list[list[list[str]]], target:str=''):\n",
    "    com:defaultdict[str, defaultdict[str, int]] = defaultdict(lambda: defaultdict(int))\n",
    "    count_all = Counter()\n",
    "    count_all1 = Counter()\n",
    "\n",
    "    is_target_specific:bool = len(target) > 0\n",
    "\n",
    "    uniqueList = []\n",
    "    for _array in iterable:\n",
    "        for line in _array:\n",
    "            for word in line:\n",
    "                if not is_target_specific:\n",
    "                    if word not in uniqueList:\n",
    "                        uniqueList.append(word)\n",
    "\n",
    "            terms_bigram = bigrams(line)\n",
    "            # Update the counter\n",
    "            count_all.update(line)\n",
    "            count_all1.update(terms_bigram)\n",
    "\n",
    "            # Build co-occurrence matrix\n",
    "            for i in range(len(line) - 1):\n",
    "                for j in range(i + 1, len(line)):\n",
    "                    w1, w2 = sorted([line[i], line[j]])\n",
    "                    if w1 != w2:\n",
    "                        com[w1][w2] += 1\n",
    "\n",
    "\n",
    "\n",
    "    com_max = []\n",
    "    # For each term, look for the most common co-occurrent terms\n",
    "    for t1 in com:\n",
    "        t1_max_terms = sorted(com[t1].items(), key=operator.itemgetter(1), reverse=True)[:5]\n",
    "        for t2, t2_count in t1_max_terms:\n",
    "            # If target is provided, only show co-occurrences with target, else all co-occurrences\n",
    "            if (is_target_specific and (target == t1 or target == t2)) or not is_target_specific:\n",
    "                if t1 not in uniqueList:\n",
    "                    uniqueList.append(t1)\n",
    "                if t2 not in uniqueList:\n",
    "                    uniqueList.append(t2)\n",
    "                com_max.append(((t1, t2), t2_count))\n",
    "    # Get the most frequent co-occurrences\n",
    "    terms_max = sorted(com_max, key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    return com, terms_max, uniqueList\n",
    "\n",
    "result_cooc = computeCooccurence([tqdm((d.split(' ') for d in documents), total = len(documents)),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cooc_result = result_cooc[0]\n",
    "import sys\n",
    "sys.getsizeof(cooc_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(cooc_result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class WordDict(dict):\n",
    "    # wordDict.putIfAbsent(string) -> int\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._index = 0\n",
    "        self.reverse = {}\n",
    "    def _setreverse(self, index, key):\n",
    "        self.reverse[index] = key\n",
    "    def __missing__(self, key):\n",
    "        super().__setitem__(key, self._index)\n",
    "        self._setreverse(self._index, key)\n",
    "        self._index += 1\n",
    "        return self[key]\n",
    "    def __getitem__(self, key):\n",
    "        # fallback to __missing__\n",
    "        if key not in self:\n",
    "            self[key] = self._index\n",
    "            self._setreverse(self._index, key)\n",
    "            self._index += 1\n",
    "        return super().__getitem__(key)\n",
    "    def getKey(self, index):\n",
    "        return self.reverse[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert to sparse matrix, then to csr matrix\n",
    "# {key1 : {key2 : count}} structure -> (key1, key2, count) structure\n",
    "# we will filter keys\n",
    "from functools import lru_cache\n",
    "\n",
    "def convert_2d_json(structure, predicate= lambda x : True):\n",
    "    wordIndexes = WordDict()\n",
    "    result = []\n",
    "    tqdm._instances.clear()\n",
    "    for key1 in tqdm(structure.keys()):\n",
    "        if not predicate(key1):\n",
    "            continue\n",
    "        idx1 = wordIndexes[key1]\n",
    "        for key2 in structure[key1].keys():\n",
    "            if predicate(key2) and structure[key1][key2] > 0:\n",
    "                idx2 = wordIndexes[key2]\n",
    "                result.append((idx1, idx2, structure[key1][key2]))\n",
    "    return result, wordIndexes\n",
    "# lets exclude strings with all numeric characters such as '000' and strings and only english characters\n",
    "@lru_cache(maxsize=70051)\n",
    "def filterfunc(x):\n",
    "    alphabet_re = re.compile('[a-zA-Z]+')\n",
    "    return not x.isnumeric() and not alphabet_re.match(x)\n",
    "\n",
    "sparse, wordIndexes = convert_2d_json(cooc_result, filterfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# find max index\n",
    "def reverse_dict(d):\n",
    "    return {v: k for k, v in d.items()}\n",
    "\n",
    "indexToWord = reverse_dict(wordIndexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Now we will convert to csr matrix\n",
    "# We need 'dictionary' to convert string to integer\n",
    "len(wordIndexes)\n",
    "import scipy.sparse as sp\n",
    "# sp.csr_matrix(data, (row, col)) -> matrix\n",
    "# (row, col, data) -> csr_matrix\n",
    "arr = np.array(sparse)\n",
    "maxidx = max(arr[:,0].max(), arr[:,1].max())\n",
    "mat = sp.csr_matrix((arr[:,2], (arr[:,0], arr[:,1])), shape=(maxidx+1, maxidx+1))\n",
    "# check if it is symmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# analyze zipf's law with word counter\n",
    "from analyzeZipf import plot_zipf\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plot_zipf(result, title=\"Zipf's law for all nouns\", prune=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'가나다'.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check if it is symmetric\n",
    "# sort by count and get submatrix\n",
    "def getSubMatrix(array, n, maxCount = 1e5):\n",
    "    subarray = array[array[:,2] < maxCount]\n",
    "    # sort by mat[:,2], then get first n elements\n",
    "    # then get mat[:,0] and mat[:,1]\n",
    "    sorted_idx = np.argsort(subarray[:,2])\n",
    "    return subarray[sorted_idx[-n:], :]\n",
    "\n",
    "top_100 = getSubMatrix(arr, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# using indexToWord, parse it to string\n",
    "def parseIndexToWord(array, indexDict):\n",
    "    result = []\n",
    "    for row in array:\n",
    "        result.append((indexDict[row[0]], indexDict[row[1]], row[2]))\n",
    "    return result\n",
    "\n",
    "# histogram of counts\n",
    "def getHistogram(array):\n",
    "    return np.histogram(array[:,2], bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "getHistogram(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parseIndexToWord(getSubMatrix(arr, 10,10000), indexToWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dump\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"word_hist.pkl\"):\n",
    "    cv = CountVectorizer(max_features=250)\n",
    "    cv_fit = cv.fit_transform(documents)\n",
    "    word_list = cv.get_feature_names_out()\n",
    "    count_list = cv_fit.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert all(isinstance(a, str) for a in documents)\n",
    "assert all(len(a) > 0 for a in documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_hist = dict(zip(word_list, count_list.tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dump word_hist\n",
    "import pickle\n",
    "import os\n",
    "if not os.path.exists('word_hist.pkl'):\n",
    "    with open('word_hist.pkl', 'wb') as f:\n",
    "        pickle.dump(word_hist, f)\n",
    "\n",
    "# reload\n",
    "with open('word_hist.pkl', 'rb') as f:\n",
    "    word_hist = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def calculateCooccurrence(self:ptm.CooccurrenceManager, iterable):\n",
    "    count = {}  # 동시출현 빈도가 저장될 dict\n",
    "    words = list(set(iterable))  # 단어별로 분리한 것을 set에 넣어 중복 제거하고, 다시 list로 변경\n",
    "    wids = [self.getIdOrAdd(w) for w in tqdm(words)]\n",
    "    for i, a in enumerate(tqdm(wids)):\n",
    "        for b in wids[i + 1:]:\n",
    "            if a == b: continue  # 같은 단어의 경우는 세지 않음\n",
    "            if a > b: a, b = b, a  # A, B와 B, A가 다르게 세어지는것을 막기 위해 항상 a < b로 순서 고정\n",
    "            count[a, b] = count.get((a, b), 0) + 1  # 실제로 센다\n",
    "\n",
    "    sorted = []\n",
    "    for tup in tqdm(count):\n",
    "        freq = count[tup]\n",
    "        left_word = self.getWord(count[0])\n",
    "        right_word = self.getWord(count[1])\n",
    "        sorted.append(((left_word, right_word), freq))\n",
    "    return sorted, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists('documents_export.csv'):\n",
    "    print(\"processing documents to csv file\")\n",
    "    with open('documents_export.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for doc in tqdm(documents):\n",
    "            split_line = doc.split(' ')\n",
    "            # remove 0-1 length words, and remove words with alphabet or which contains number\n",
    "            # remove some stopwords\n",
    "            alphabet = re.compile('[a-zA-Z]')\n",
    "            number = re.compile('[0-9]')\n",
    "            split_line = [word for word in split_line if len(word) > 1 and not alphabet.search(word) and not number.search(word)]\n",
    "            if len(split_line) > 0:\n",
    "                writer.writerow(split_line)\n",
    "\n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document_search(documents, lambda x: '인도' in x and '태평양' in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document_search(documents, lambda x: '농협' in x and '일본' in x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T10:33:50.051114Z",
     "start_time": "2023-05-03T10:33:50.002893Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T10:19:42.639665Z",
     "start_time": "2023-05-03T10:19:42.586217Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
